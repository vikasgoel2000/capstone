library(tm)
library(SnowballC)

setwd("C:/Users/Vikas/Downloads/nfl-dataset-")
data<-read.csv("NFL_SocialMedia_sample_data1.csv",header=T)



log<-data[1]

corpus = Corpus(VectorSource(log$content))

corpus[[1]]


corpus <- tm_map(corpus, tolower)
corpus[[1]]
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removePunctuation)
corpus[[1]]
corpus = tm_map(corpus, removeNumbers)
corpus[[1]]
corpus <- tm_map(corpus, stripWhitespace)
corpus[[2]]

corpus <- tm_map(corpus, PlainTextDocument)

dtm = DocumentTermMatrix(corpus)

dtm
findFreqTerms(dtm, 10)
findFreqTerms(dtm, 5)


dtm_tfxidf <- weightTfIdf(dtm)

dtm_tfxidf

inspect(dtm_tfxidf[1:5, 1:100])

m<-as.matrix(dtm_tfxidf)

rownames(m)

rownames(m)<-1:nrow(m)

rownames(m)

norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)

m_norm <- norm_eucl(m)

cl <- kmeans(m_norm, 10)

cl

cl$cluster

cl$size

cluster_out<-cbind(log,cl$cluster)

colnames(cluster_out)<-c("content","cluster")

View(cluster_out)


write.csv(cluster_out,"cluster_out.csv")



col.names<-c("Word_Count","cluster","word_rank","Top_Words")
c<-read.table(text="",col.names=col.names)

for (i in 1:10)
{
data1<-subset(cluster_out,cluster==i)
corpus1 = Corpus(VectorSource(data1$content))
corpus1 <- tm_map(corpus1, tolower)
corpus1 = tm_map(corpus1, removeWords, stopwords("english"))
corpus1 = tm_map(corpus1, removePunctuation)
corpus1 = tm_map(corpus1, removeNumbers)
corpus1 <- tm_map(corpus1, stripWhitespace)
corpus1 <- tm_map(corpus1, PlainTextDocument)
dtm1 = DocumentTermMatrix(corpus1)

freq <- colSums(as.matrix(dtm1))
ord <- order(freq)


a1<-as.matrix(freq[tail(ord,5)])

a1<-as.data.frame(a1)
a1$cluster<-i
a1$word_rank<-c(1,2,3,4,5)
a1$TopWords<-rownames(a1)
rownames(a1)<-NULL
colnames(a1)<-c("Word_Count","cluster","word_rank","Top_Words") 
c<-rbind(c,a1)
rm(data1)
rm(corpus1)
rm(a1)
}
c<-c[c(2,3,4,1)]

write.csv(c,"TopWords.csv")
