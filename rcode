
setwd("C:/Users/Vikas/Downloads/Coursera-SwiftKey/final/en_US")
mydata<-read.table("en_US.twitter.txt")
library(tm)
mydata<-system.file("texts","en_US.twitter.txt",package="tm")
setwd("C:/Users/Vikas/Downloads")
mydata<-system.file("texts","Coursera-SwiftKey.zip",package="tm")
?system.file()
setwd("C:/Users/Vikas/Downloads/Coursera-SwiftKey/final/en_US")
mydata<-readLines("en_US.twitter.txt",encoding="UTF-8")
is.utf8("en_US.twitter.txt")
str(mydata)
summary(mydata)
mydata1<-readLines("en_US.blogs.txt",encoding="UTF-8")
scan("en_US.blogs.txt", character(0), sep = "\n")
mydata1<-scan("en_US.blogs.txt", character(0), sep = "\n")
count,chars(mydata1)
count.chars(mydata1)
install.packages("parser")
count.chars(mydata1)
file.show(mydata)
nlines(mydata1)
?nlines()
install.packages("R.utils")
count.chars(mydata1)
nchar(mydata1)
max(nchar(mydata1))
max(nchar(mydata))
mydata2<-readLines("en_US.news.txt",encoding="UTF-8")
max(nchar(mydata2))
tolower(mydata)
mydata[1]
mydata[2]
mydata<-tm_map(mydata, tolower)
install.packages("cwhmisc")
tolower(mydata)
tolower(mydata1)
tolower(mydata2)
mydata2<-tolower(mydata2)
length(grep("love",mydata2))
grep("biostats",mydata)
mydata[556872]
length(grep("biostats",mydata))
length(grep("love",mydata))
length(grep("hate",mydata))
90956/22138
length(grep("A computer once beat me at chess, but it was no match for me at kickboxing",mydata))
save.image("C:/Users/Vikas/Downloads/Coursera-SwiftKey/final/en_US/data.Rdata.RData")
savehistory("C:/Users/Vikas/Downloads/Coursera-SwiftKey/final/en_US/project.r")

=========================================================
setwd("C:/Users/Vikas/Downloads/Coursera-SwiftKey/final/en_US")
library(tm)
library(SnowballC)
library(RWeka)
twitter<-scan("en_US.twitter.txt", character(0), sep = "\n",n=1000)
tw<-as.data.frame(twitter,stringsAsFactors=FALSE)
corpus = Corpus(VectorSource(tw$twitter))
corpus <- tm_map(corpus, content_transformer(tolower))

corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))


library("stringi")
corpus <- tm_map(corpus, function(x) gsub("[!?,.]+", ".", x))
corpus <- tm_map(corpus, function(x) gsub('[])(;:#%$^*\\~{}[&+=@/"`|<>_]+', "", x))
corpus2 <- tm_map(corpus2, removeNumbers)
corpus2 <- tm_map(corpus2, stripWhitespace)

corpus2 <- tm_map(corpus2, function(x) strsplit(x, "\\."))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus2, control = list(tokenize = BigramTokenizer))

library("qdap") 
aslines <- as.list(corpus2)
data_table <- matrix(unlist(aslines), ncol = 1, byrow = TRUE)
corpus3 = Corpus(VectorSource(data_table))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus2, control = list(tokenize = BigramTokenizer))
==========================================================================================================

setwd("C:/Users/Vikas/Downloads/Coursera-SwiftKey/final/en_US")
library(tm)
library(SnowballC)
library(RWeka)
library("stringi")
library("qdap") 
twitter<-scan("en_US.twitter.txt", character(0), sep = "\n",n=1000)
tw<-as.data.frame(twitter,stringsAsFactors=FALSE)
corpus = Corpus(VectorSource(tw$twitter))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, function(x) gsub("[!?,.]+", ".", x))
corpus <- tm_map(corpus, function(x) gsub('[])(;:#%$^*\\~{}[&+=@/"`|<>_]+', "", x))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removePunctuation)
aslines <- as.list(corpus2)
data_table <- matrix(unlist(aslines), ncol = 1, byrow = TRUE)
corpus1 = Corpus(VectorSource(data_table))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus1, control = list(tokenize = BigramTokenizer))
matrix<-data.matrix(tdm)
n<-ncol(matrix)
summed<-rowSums(matrix1[,c(1:n)])
summed<-as.data.frame(summed)
summed$gram<-rownames(summed)
summed<-data.frame(summed,row.names=NULL)
=================================================
str<-"  How many words are in this      sentence"
m <- str_match_all( str, "\\S+" )
m<-data.frame(m)
count<-length(m[[1]])
find<-paste(m[(count-1):count,], collapse=" ")


